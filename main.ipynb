{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.5 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/123.5 kB ? eta -:--:--\n",
      "     --------------------- --------------- 71.7/123.5 kB 787.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 123.5/123.5 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\matti\\miniconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/b8/ad/3398312096118c4e62a5827664e52a04d5068e84d04142dd4a0da8a567ae/regex-2023.10.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\matti\\miniconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/be/5f/2cc4f229bf85d90842f513be31a529595c10b8c8b8193c077230a8c17548/tokenizers-0.15.0-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/a5/dd/85bcfeab1451eeb24278575fa1b8f24bea7a99c5ff0bb34af63dfe20e772/safetensors-0.4.1-cp311-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp311-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\matti\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.7/7.9 MB 14.4 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.9/7.9 MB 20.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.1/7.9 MB 21.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.1/7.9 MB 23.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.4/7.9 MB 24.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.3/7.9 MB 23.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.7/7.9 MB 24.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 23.0 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp311-cp311-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.6/269.6 kB 16.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp311-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 277.5/277.5 kB 16.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 39.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 34.7 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2023.10.3 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n",
      "Requirement already satisfied: numba in c:\\users\\matti\\miniconda3\\lib\\site-packages (0.58.1)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from numba) (0.41.1)\n",
      "Requirement already satisfied: numpy<1.27,>=1.22 in c:\\users\\matti\\miniconda3\\lib\\site-packages (from numba) (1.26.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install numba\n",
    "!pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from numba import cuda\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TFAutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import create_optimizer\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "import torch.cuda\n",
    "from typing import Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_df(path: str = None, sample: int = 50) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, encoding=\"ISO-8859-1\")\n",
    "    df.loc[df['outcome_class'] == 't', 'outcome_class'] = 'T'\n",
    "    df.loc[df['outcome_class'] == 'd', 'outcome_class'] = 'F'\n",
    "    df['q1'] = df['q1'].apply(lambda x: x.replace('\\n', ''))\n",
    "    df = df.rename(columns={'q1': 'sent', 'outcome_class': 'labels'})\n",
    "    df = df[['sent', 'labels']]\n",
    "    if sample != 0:\n",
    "        return df.iloc[:sample, :]\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "def create_dataset_with_crossval(df: pd.DataFrame = None, seed: int = 42, cv: int = 2) \\\n",
    "        -> Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n",
    "    train = {f'split_{i + 1}': [] for i in range(cv)}\n",
    "    test = {f'split_{i + 1}': [] for i in range(cv)}\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    in_arr = np.arange(0, len(df))\n",
    "    np.random.shuffle(in_arr)\n",
    "    in_arr_r = in_arr.reshape((cv, int(len(df) / cv)))  # reshape data (num cv, total_rows/num cv)\n",
    "\n",
    "    cv_ind = 0\n",
    "    for k in train.keys():  # train.keys() = split_1, split_2 ...\n",
    "\n",
    "        ind = in_arr_r[cv_ind, :]  # test indices\n",
    "        antind = [i for i in in_arr if i not in ind]  # train indices\n",
    "\n",
    "        intent_train = df[df.index.isin(antind)]\n",
    "        intent_test = df[df.index.isin(ind)]\n",
    "\n",
    "        train[k].append(intent_train)\n",
    "        test[k].append(intent_test)\n",
    "\n",
    "        train[k] = pd.concat(train[k])\n",
    "        test[k] = pd.concat(test[k])\n",
    "\n",
    "        cv_ind += 1\n",
    "\n",
    "    # train,test: dicts with num cv splits, each split has len(data)*(1-1/cv), len(data)*1/cv\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def preprocess_function(examples: datasets.Dataset) -> datasets.Dataset:\n",
    "    inputs = [ex for ex in examples[\"sent\"]]\n",
    "    targets = [ex for ex in examples[\"labels\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/sign_events_data_statements.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matti\\Documents\\GitHub\\Cognitive_project\\main.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matti/Documents/GitHub/Cognitive_project/main.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mseed(seed)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matti/Documents/GitHub/Cognitive_project/main.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m numcv \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matti/Documents/GitHub/Cognitive_project/main.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m intent_df \u001b[39m=\u001b[39m csv_to_df(\u001b[39m'\u001b[39;49m\u001b[39mdata/sign_events_data_statements.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matti/Documents/GitHub/Cognitive_project/main.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train, test \u001b[39m=\u001b[39m create_dataset_with_crossval(df\u001b[39m=\u001b[39mintent_df, seed\u001b[39m=\u001b[39mseed, cv\u001b[39m=\u001b[39mnumcv)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matti/Documents/GitHub/Cognitive_project/main.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m results \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;32mc:\\Users\\matti\\Documents\\GitHub\\Cognitive_project\\main.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matti/Documents/GitHub/Cognitive_project/main.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcsv_to_df\u001b[39m(path: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, sample: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matti/Documents/GitHub/Cognitive_project/main.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(path, encoding\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mISO-8859-1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matti/Documents/GitHub/Cognitive_project/main.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df\u001b[39m.\u001b[39mloc[df[\u001b[39m'\u001b[39m\u001b[39moutcome_class\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moutcome_class\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mT\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matti/Documents/GitHub/Cognitive_project/main.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     df\u001b[39m.\u001b[39mloc[df[\u001b[39m'\u001b[39m\u001b[39moutcome_class\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39md\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moutcome_class\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mF\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\matti\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\matti\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\matti\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\matti\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\matti\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/sign_events_data_statements.csv'"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "numcv = 10\n",
    "intent_df = csv_to_df('data/sign_events_data_statements.csv')\n",
    "train, test = create_dataset_with_crossval(df=intent_df, seed=seed, cv=numcv)\n",
    "\n",
    "results = {}\n",
    "preds = {}\n",
    "trus = {}\n",
    "collect_result = {}\n",
    "model_size = 'small'\n",
    "num_epochs = 3\n",
    "\n",
    "for sp in tqdm(train.keys()):\n",
    "\n",
    "    data_train = Dataset.from_pandas(train[sp])\n",
    "    data_test = Dataset.from_pandas(test[sp])\n",
    "\n",
    "    checkpoint = f\"google/flan-t5-{model_size}\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "    data_train = data_train.map(preprocess_function, batched=True)\n",
    "    data_test = data_test.map(preprocess_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f\"{model_size}_scenario_intent\",\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=2,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=num_epochs,\n",
    "        predict_with_generate=True,\n",
    "        fp16=False,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=data_train,\n",
    "        eval_dataset=data_test,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    pred = []\n",
    "    for i in test[sp]['sent']:\n",
    "        toki = tokenizer(i, return_tensors=\"pt\").input_ids   # return token id's as pytorch tensor\n",
    "        h = model.generate(toki, return_dict_in_generate=True, output_scores=True)\n",
    "        decoded_preds = tokenizer.batch_decode(h.sequences, skip_special_tokens=True)\n",
    "        pred.append(decoded_preds[0])\n",
    "\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    preds[sp] = pred\n",
    "    trus[sp] = np.array(test[sp]['labels'])\n",
    "\n",
    "    collect_result[sp] = pd.DataFrame.from_dict(\n",
    "        {\"prediction\": pred,\n",
    "         'labels': np.array(test[sp]['labels'])})\n",
    "\n",
    "    collect_result[sp]['corr'] = collect_result[sp]['prediction'] == collect_result[sp]['labels']\n",
    "    results[sp] = [(pred == trus[sp]).mean()]\n",
    "    print(\"collect results:\",collect_result[sp].head(20))\n",
    "    del model\n",
    "    del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame.from_dict(results)\n",
    "print('Average accuracy: ', results.mean(axis=1))\n",
    "print('Max accuracy: ', results.max(axis=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
