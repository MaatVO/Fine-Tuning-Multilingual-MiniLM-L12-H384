
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'eval_loss': 2.5373761653900146, 'eval_runtime': 0.1763, 'eval_samples_per_second': 28.359, 'eval_steps_per_second': 5.672, 'epoch': 1.0}
{'eval_loss': 1.611716628074646, 'eval_runtime': 0.158, 'eval_samples_per_second': 31.646, 'eval_steps_per_second': 6.329, 'epoch': 2.0}
{'eval_loss': 1.2798974514007568, 'eval_runtime': 0.2759, 'eval_samples_per_second': 18.121, 'eval_steps_per_second': 3.624, 'epoch': 3.0}
{'train_runtime': 21.9171, 'train_samples_per_second': 6.16, 'train_steps_per_second': 3.148, 'train_loss': 2.820955138275589, 'epoch': 3.0}

  0%|          | 0/10 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'eval_loss': 2.5373761653900146, 'eval_runtime': 0.1479, 'eval_samples_per_second': 33.796, 'eval_steps_per_second': 6.759, 'epoch': 1.0}

  0%|          | 0/10 [00:14<?, ?it/s]
{'eval_loss': 1.611716628074646, 'eval_runtime': 0.1578, 'eval_samples_per_second': 31.693, 'eval_steps_per_second': 6.339, 'epoch': 2.0}
{'eval_loss': 1.2798974514007568, 'eval_runtime': 0.1485, 'eval_samples_per_second': 33.663, 'eval_steps_per_second': 6.733, 'epoch': 3.0}

  0%|          | 0/10 [00:19<?, ?it/s]

  0%|          | 0/10 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'eval_loss': 2.5373761653900146, 'eval_runtime': 0.1436, 'eval_samples_per_second': 34.83, 'eval_steps_per_second': 6.966, 'epoch': 1.0}
  0%|          | 0/10 [00:07<?, ?it/s]

  0%|          | 0/10 [00:12<?, ?it/s]
{'eval_loss': 1.2798974514007568, 'eval_runtime': 0.207, 'eval_samples_per_second': 24.153, 'eval_steps_per_second': 4.831, 'epoch': 3.0}
{'train_runtime': 15.1812, 'train_samples_per_second': 8.893, 'train_steps_per_second': 4.545, 'train_loss': 2.820955138275589, 'epoch': 3.0}
collect results:    prediction labels   corr
0           a      T  False
1  Cork tiles      T  False
2           T      T   True
3           T      T   True
  0%|          | 0/10 [00:17<?, ?it/s]c:\Users\matti\Documents\GitHub\Fine_tuning_llm_project\.env\Lib\site-packages\transformers\generation\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(

 10%|█         | 1/10 [00:18<02:45, 18.33s/it]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'eval_loss': 2.7409303188323975, 'eval_runtime': 0.1521, 'eval_samples_per_second': 32.878, 'eval_steps_per_second': 6.576, 'epoch': 1.0}
 10%|█         | 1/10 [00:25<02:45, 18.33s/it]

 10%|█         | 1/10 [00:31<02:45, 18.33s/it]
{'eval_loss': 1.546657681465149, 'eval_runtime': 0.1392, 'eval_samples_per_second': 35.932, 'eval_steps_per_second': 7.186, 'epoch': 3.0}
{'train_runtime': 15.4978, 'train_samples_per_second': 8.711, 'train_steps_per_second': 4.452, 'train_loss': 2.7482102988422783, 'epoch': 3.0}
collect results:                                           prediction labels   corr
0                   i will be announcing the wedding      T  False
1                                                  T      T   True
2                                                  T      T   True
3                                                  T      T   True
 10%|█         | 1/10 [00:36<02:45, 18.33s/it]c:\Users\matti\Documents\GitHub\Fine_tuning_llm_project\.env\Lib\site-packages\transformers\generation\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(

 20%|██        | 2/10 [00:37<02:28, 18.58s/it]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'eval_loss': 2.0672383308410645, 'eval_runtime': 0.1182, 'eval_samples_per_second': 42.299, 'eval_steps_per_second': 8.46, 'epoch': 1.0}
 20%|██        | 2/10 [00:44<02:28, 18.58s/it]

 20%|██        | 2/10 [00:50<03:21, 25.21s/it]

  0%|          | 0/10 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'eval_loss': 2.5373761653900146, 'eval_runtime': 0.4015, 'eval_samples_per_second': 12.454, 'eval_steps_per_second': 2.491, 'epoch': 1.0}
  0%|          | 0/10 [00:13<?, ?it/s]

  0%|          | 0/10 [00:24<?, ?it/s]
{'eval_loss': 1.2798974514007568, 'eval_runtime': 0.3651, 'eval_samples_per_second': 13.694, 'eval_steps_per_second': 2.739, 'epoch': 3.0}
{'train_runtime': 33.28, 'train_samples_per_second': 4.056, 'train_steps_per_second': 2.073, 'train_loss': 2.820955138275589, 'epoch': 3.0}
collect results:    prediction labels   corr
0           a      T  False
1  Cork tiles      T  False
2           T      T   True
3           T      T   True
  0%|          | 0/10 [00:35<?, ?it/s]c:\Users\matti\Documents\GitHub\Fine_tuning_llm_project\.env\Lib\site-packages\transformers\generation\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(

 10%|█         | 1/10 [00:36<05:28, 36.55s/it]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'eval_loss': 2.7409303188323975, 'eval_runtime': 0.5205, 'eval_samples_per_second': 9.607, 'eval_steps_per_second': 1.921, 'epoch': 1.0}
 10%|█         | 1/10 [00:54<05:28, 36.55s/it]

 10%|█         | 1/10 [01:08<05:28, 36.55s/it]
{'eval_loss': 1.546657681465149, 'eval_runtime': 0.5225, 'eval_samples_per_second': 9.569, 'eval_steps_per_second': 1.914, 'epoch': 3.0}
 10%|█         | 1/10 [01:22<05:28, 36.55s/it]c:\Users\matti\Documents\GitHub\Fine_tuning_llm_project\.env\Lib\site-packages\transformers\generation\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
collect results:                                           prediction labels   corr
0                   i will be announcing the wedding      T  False
1                                                  T      T   True
2                                                  T      T   True
3                                                  T      T   True
4  I am a huge 49ers fan so I I very excited abou...      T  False

 20%|██        | 2/10 [01:23<05:42, 42.79s/it]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'eval_loss': 2.0672383308410645, 'eval_runtime': 0.7008, 'eval_samples_per_second': 7.135, 'eval_steps_per_second': 1.427, 'epoch': 1.0}
 20%|██        | 2/10 [01:45<05:42, 42.79s/it]

 20%|██        | 2/10 [02:03<05:42, 42.79s/it]
{'eval_loss': 0.8450724482536316, 'eval_runtime': 0.702, 'eval_samples_per_second': 7.123, 'eval_steps_per_second': 1.425, 'epoch': 3.0}
{'train_runtime': 55.475, 'train_samples_per_second': 2.434, 'train_steps_per_second': 1.244, 'train_loss': 2.815945169200068, 'epoch': 3.0}
collect results:       prediction labels   corr
0  Cannock Chase      T  False
1              T      T   True
2              T      T   True
3              T      T   True
 20%|██        | 2/10 [02:21<05:42, 42.79s/it]c:\Users\matti\Documents\GitHub\Fine_tuning_llm_project\.env\Lib\site-packages\transformers\generation\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(

 30%|███       | 3/10 [02:22<05:49, 49.96s/it]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'eval_loss': 2.756089687347412, 'eval_runtime': 1.1842, 'eval_samples_per_second': 4.222, 'eval_steps_per_second': 0.844, 'epoch': 1.0}

