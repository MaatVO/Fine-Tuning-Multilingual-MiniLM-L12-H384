
You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 0.6882, 'learning_rate': 1.9333333333333333e-05, 'epoch': 1.0}
bert.embeddings.word_embeddings.weight: Requires Grad - True
bert.embeddings.position_embeddings.weight: Requires Grad - True
bert.embeddings.token_type_embeddings.weight: Requires Grad - True
bert.embeddings.LayerNorm.weight: Requires Grad - True
bert.embeddings.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.0.attention.self.query.weight: Requires Grad - False
bert.encoder.layer.0.attention.self.query.bias: Requires Grad - False
bert.encoder.layer.0.attention.self.key.weight: Requires Grad - False
bert.encoder.layer.0.attention.self.key.bias: Requires Grad - False
bert.encoder.layer.0.attention.self.value.weight: Requires Grad - False
bert.encoder.layer.0.attention.self.value.bias: Requires Grad - False
bert.encoder.layer.0.attention.output.dense.weight: Requires Grad - False
bert.encoder.layer.0.attention.output.dense.bias: Requires Grad - False
bert.encoder.layer.0.attention.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.0.attention.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.0.intermediate.dense.weight: Requires Grad - False
bert.encoder.layer.0.intermediate.dense.bias: Requires Grad - False
bert.encoder.layer.0.output.dense.weight: Requires Grad - False
bert.encoder.layer.0.output.dense.bias: Requires Grad - False
bert.encoder.layer.0.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.0.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.1.attention.self.query.weight: Requires Grad - False
bert.encoder.layer.1.attention.self.query.bias: Requires Grad - False
bert.encoder.layer.1.attention.self.key.weight: Requires Grad - False
bert.encoder.layer.1.attention.self.key.bias: Requires Grad - False
bert.encoder.layer.1.attention.self.value.weight: Requires Grad - False
bert.encoder.layer.1.attention.self.value.bias: Requires Grad - False
bert.encoder.layer.1.attention.output.dense.weight: Requires Grad - False
bert.encoder.layer.1.attention.output.dense.bias: Requires Grad - False
bert.encoder.layer.1.attention.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.1.attention.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.1.intermediate.dense.weight: Requires Grad - False
bert.encoder.layer.1.intermediate.dense.bias: Requires Grad - False
bert.encoder.layer.1.output.dense.weight: Requires Grad - False
bert.encoder.layer.1.output.dense.bias: Requires Grad - False
bert.encoder.layer.1.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.1.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.2.attention.self.query.weight: Requires Grad - False
bert.encoder.layer.2.attention.self.query.bias: Requires Grad - False
bert.encoder.layer.2.attention.self.key.weight: Requires Grad - False
bert.encoder.layer.2.attention.self.key.bias: Requires Grad - False
bert.encoder.layer.2.attention.self.value.weight: Requires Grad - False
bert.encoder.layer.2.attention.self.value.bias: Requires Grad - False
bert.encoder.layer.2.attention.output.dense.weight: Requires Grad - False
bert.encoder.layer.2.attention.output.dense.bias: Requires Grad - False
bert.encoder.layer.2.attention.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.2.attention.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.2.intermediate.dense.weight: Requires Grad - False
bert.encoder.layer.2.intermediate.dense.bias: Requires Grad - False
bert.encoder.layer.2.output.dense.weight: Requires Grad - False
bert.encoder.layer.2.output.dense.bias: Requires Grad - False
bert.encoder.layer.2.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.2.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.3.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.3.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.3.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.3.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.3.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.3.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.3.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.3.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.3.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.3.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.3.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.3.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.3.output.dense.weight: Requires Grad - True
bert.encoder.layer.3.output.dense.bias: Requires Grad - True
bert.encoder.layer.3.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.3.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.4.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.4.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.4.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.4.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.4.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.4.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.4.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.4.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.4.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.4.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.4.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.4.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.4.output.dense.weight: Requires Grad - True
bert.encoder.layer.4.output.dense.bias: Requires Grad - True
bert.encoder.layer.4.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.4.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.5.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.5.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.5.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.5.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.5.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.5.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.5.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.5.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.5.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.5.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.5.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.5.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.5.output.dense.weight: Requires Grad - True
bert.encoder.layer.5.output.dense.bias: Requires Grad - True
bert.encoder.layer.5.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.5.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.6.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.6.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.6.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.6.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.6.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.6.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.6.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.6.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.6.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.6.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.6.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.6.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.6.output.dense.weight: Requires Grad - True
bert.encoder.layer.6.output.dense.bias: Requires Grad - True
bert.encoder.layer.6.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.6.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.7.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.7.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.7.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.7.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.7.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.7.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.7.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.7.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.7.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.7.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.7.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.7.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.7.output.dense.weight: Requires Grad - True
bert.encoder.layer.7.output.dense.bias: Requires Grad - True
bert.encoder.layer.7.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.7.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.8.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.8.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.8.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.8.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.8.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.8.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.8.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.8.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.8.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.8.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.8.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.8.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.8.output.dense.weight: Requires Grad - True
bert.encoder.layer.8.output.dense.bias: Requires Grad - True
bert.encoder.layer.8.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.8.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.9.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.9.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.9.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.9.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.9.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.9.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.9.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.9.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.9.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.9.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.9.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.9.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.9.output.dense.weight: Requires Grad - True
bert.encoder.layer.9.output.dense.bias: Requires Grad - True
bert.encoder.layer.9.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.9.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.10.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.10.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.10.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.10.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.10.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.10.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.10.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.10.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.10.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.10.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.10.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.10.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.10.output.dense.weight: Requires Grad - True
bert.encoder.layer.10.output.dense.bias: Requires Grad - True
bert.encoder.layer.10.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.10.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.11.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.11.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.11.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.11.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.11.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.11.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.11.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.11.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.11.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.11.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.11.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.11.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.11.output.dense.weight: Requires Grad - True
bert.encoder.layer.11.output.dense.bias: Requires Grad - True
bert.encoder.layer.11.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.11.output.LayerNorm.bias: Requires Grad - True
bert.pooler.dense.weight: Requires Grad - True
bert.pooler.dense.bias: Requires Grad - True
classifier.weight: Requires Grad - True
classifier.bias: Requires Grad - True
{'loss': 0.6574, 'learning_rate': 1.9333333333333333e-05, 'epoch': 1.0}
C:\Users\matti\AppData\Local\Temp\ipykernel_4564\1102219850.py:18: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric[met] = load_metric(met)
{'eval_loss': 0.6282761693000793, 'eval_accuracy': 0.675, 'eval_recall': 0.6842105263157895, 'eval_precision': 0.65, 'eval_f1': 0.6666666666666667, 'eval_runtime': 11.137, 'eval_samples_per_second': 3.592, 'eval_steps_per_second': 0.269, 'epoch': 1.0}
{'loss': 0.5826, 'learning_rate': 1.866666666666667e-05, 'epoch': 2.0}
{'eval_loss': 0.613959550857544, 'eval_accuracy': 0.675, 'eval_recall': 0.7368421052631579, 'eval_precision': 0.6363636363636364, 'eval_f1': 0.6829268292682926, 'eval_runtime': 8.0813, 'eval_samples_per_second': 4.95, 'eval_steps_per_second': 0.371, 'epoch': 2.0}
{'loss': 0.5061, 'learning_rate': 1.8e-05, 'epoch': 3.0}
{'eval_loss': 0.6039782762527466, 'eval_accuracy': 0.675, 'eval_recall': 0.6842105263157895, 'eval_precision': 0.65, 'eval_f1': 0.6666666666666667, 'eval_runtime': 8.1435, 'eval_samples_per_second': 4.912, 'eval_steps_per_second': 0.368, 'epoch': 3.0}
{'loss': 0.4632, 'learning_rate': 1.7333333333333336e-05, 'epoch': 4.0}
{'eval_loss': 0.6225079298019409, 'eval_accuracy': 0.725, 'eval_recall': 0.7368421052631579, 'eval_precision': 0.7, 'eval_f1': 0.717948717948718, 'eval_runtime': 8.7324, 'eval_samples_per_second': 4.581, 'eval_steps_per_second': 0.344, 'epoch': 4.0}
{'loss': 0.4061, 'learning_rate': 1.6666666666666667e-05, 'epoch': 5.0}
{'eval_loss': 0.6435099244117737, 'eval_accuracy': 0.725, 'eval_recall': 0.8421052631578947, 'eval_precision': 0.6666666666666666, 'eval_f1': 0.744186046511628, 'eval_runtime': 8.1168, 'eval_samples_per_second': 4.928, 'eval_steps_per_second': 0.37, 'epoch': 5.0}
{'loss': 0.3799, 'learning_rate': 1.6000000000000003e-05, 'epoch': 6.0}
{'eval_loss': 0.5827082395553589, 'eval_accuracy': 0.775, 'eval_recall': 0.8947368421052632, 'eval_precision': 0.7083333333333334, 'eval_f1': 0.7906976744186046, 'eval_runtime': 8.1738, 'eval_samples_per_second': 4.894, 'eval_steps_per_second': 0.367, 'epoch': 6.0}
{'loss': 0.3559, 'learning_rate': 1.5333333333333334e-05, 'epoch': 7.0}
{'eval_loss': 0.6019521951675415, 'eval_accuracy': 0.775, 'eval_recall': 0.8947368421052632, 'eval_precision': 0.7083333333333334, 'eval_f1': 0.7906976744186046, 'eval_runtime': 8.8064, 'eval_samples_per_second': 4.542, 'eval_steps_per_second': 0.341, 'epoch': 7.0}
{'loss': 0.3, 'learning_rate': 1.4666666666666666e-05, 'epoch': 8.0}
{'eval_loss': 0.715148389339447, 'eval_accuracy': 0.725, 'eval_recall': 0.7368421052631579, 'eval_precision': 0.7, 'eval_f1': 0.717948717948718, 'eval_runtime': 8.1375, 'eval_samples_per_second': 4.916, 'eval_steps_per_second': 0.369, 'epoch': 8.0}
{'loss': 0.2876, 'learning_rate': 1.4e-05, 'epoch': 9.0}
{'eval_loss': 0.6194847822189331, 'eval_accuracy': 0.775, 'eval_recall': 0.8947368421052632, 'eval_precision': 0.7083333333333334, 'eval_f1': 0.7906976744186046, 'eval_runtime': 8.126, 'eval_samples_per_second': 4.922, 'eval_steps_per_second': 0.369, 'epoch': 9.0}
{'loss': 0.2596, 'learning_rate': 1.3333333333333333e-05, 'epoch': 10.0}
{'eval_loss': 0.7156305313110352, 'eval_accuracy': 0.725, 'eval_recall': 0.7368421052631579, 'eval_precision': 0.7, 'eval_f1': 0.717948717948718, 'eval_runtime': 8.7916, 'eval_samples_per_second': 4.55, 'eval_steps_per_second': 0.341, 'epoch': 10.0}
{'loss': 0.2455, 'learning_rate': 1.2666666666666667e-05, 'epoch': 11.0}
{'eval_loss': 0.6453939080238342, 'eval_accuracy': 0.775, 'eval_recall': 0.8947368421052632, 'eval_precision': 0.7083333333333334, 'eval_f1': 0.7906976744186046, 'eval_runtime': 8.4408, 'eval_samples_per_second': 4.739, 'eval_steps_per_second': 0.355, 'epoch': 11.0}
{'loss': 0.2277, 'learning_rate': 1.2e-05, 'epoch': 12.0}
{'eval_loss': 0.7650105953216553, 'eval_accuracy': 0.75, 'eval_recall': 0.8421052631578947, 'eval_precision': 0.6956521739130435, 'eval_f1': 0.761904761904762, 'eval_runtime': 8.2282, 'eval_samples_per_second': 4.861, 'eval_steps_per_second': 0.365, 'epoch': 12.0}
{'loss': 0.2419, 'learning_rate': 1.1333333333333334e-05, 'epoch': 13.0}
{'eval_loss': 0.8645349740982056, 'eval_accuracy': 0.725, 'eval_recall': 0.7368421052631579, 'eval_precision': 0.7, 'eval_f1': 0.717948717948718, 'eval_runtime': 9.0243, 'eval_samples_per_second': 4.432, 'eval_steps_per_second': 0.332, 'epoch': 13.0}
{'loss': 0.2148, 'learning_rate': 1.0666666666666667e-05, 'epoch': 14.0}
{'eval_loss': 0.8517316579818726, 'eval_accuracy': 0.75, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.7142857142857143, 'eval_f1': 0.7500000000000001, 'eval_runtime': 8.3834, 'eval_samples_per_second': 4.771, 'eval_steps_per_second': 0.358, 'epoch': 14.0}
{'loss': 0.1406, 'learning_rate': 1e-05, 'epoch': 15.0}
{'eval_loss': 0.8730375170707703, 'eval_accuracy': 0.75, 'eval_recall': 0.8421052631578947, 'eval_precision': 0.6956521739130435, 'eval_f1': 0.761904761904762, 'eval_runtime': 8.3062, 'eval_samples_per_second': 4.816, 'eval_steps_per_second': 0.361, 'epoch': 15.0}
{'loss': 0.1372, 'learning_rate': 9.333333333333334e-06, 'epoch': 16.0}
{'eval_loss': 0.8060147166252136, 'eval_accuracy': 0.775, 'eval_recall': 0.8947368421052632, 'eval_precision': 0.7083333333333334, 'eval_f1': 0.7906976744186046, 'eval_runtime': 9.0426, 'eval_samples_per_second': 4.424, 'eval_steps_per_second': 0.332, 'epoch': 16.0}
{'loss': 0.1253, 'learning_rate': 8.666666666666668e-06, 'epoch': 17.0}
{'eval_loss': 0.8950939178466797, 'eval_accuracy': 0.75, 'eval_recall': 0.7368421052631579, 'eval_precision': 0.7368421052631579, 'eval_f1': 0.7368421052631579, 'eval_runtime': 8.2592, 'eval_samples_per_second': 4.843, 'eval_steps_per_second': 0.363, 'epoch': 17.0}
{'loss': 0.0931, 'learning_rate': 8.000000000000001e-06, 'epoch': 18.0}
{'eval_loss': 0.9456179738044739, 'eval_accuracy': 0.75, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.7142857142857143, 'eval_f1': 0.7500000000000001, 'eval_runtime': 8.2969, 'eval_samples_per_second': 4.821, 'eval_steps_per_second': 0.362, 'epoch': 18.0}
{'loss': 0.0889, 'learning_rate': 7.333333333333333e-06, 'epoch': 19.0}
{'eval_loss': 0.9145201444625854, 'eval_accuracy': 0.775, 'eval_recall': 0.8421052631578947, 'eval_precision': 0.7272727272727273, 'eval_f1': 0.7804878048780488, 'eval_runtime': 8.8945, 'eval_samples_per_second': 4.497, 'eval_steps_per_second': 0.337, 'epoch': 19.0}
{'loss': 0.106, 'learning_rate': 6.666666666666667e-06, 'epoch': 20.0}
{'eval_loss': 0.9804417490959167, 'eval_accuracy': 0.75, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.7142857142857143, 'eval_f1': 0.7500000000000001, 'eval_runtime': 8.2707, 'eval_samples_per_second': 4.836, 'eval_steps_per_second': 0.363, 'epoch': 20.0}
{'loss': 0.0666, 'learning_rate': 6e-06, 'epoch': 21.0}
{'eval_loss': 1.083160638809204, 'eval_accuracy': 0.725, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.6818181818181818, 'eval_f1': 0.7317073170731707, 'eval_runtime': 8.2295, 'eval_samples_per_second': 4.861, 'eval_steps_per_second': 0.365, 'epoch': 21.0}
{'loss': 0.0703, 'learning_rate': 5.333333333333334e-06, 'epoch': 22.0}
{'eval_loss': 1.0646564960479736, 'eval_accuracy': 0.75, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.7142857142857143, 'eval_f1': 0.7500000000000001, 'eval_runtime': 8.8111, 'eval_samples_per_second': 4.54, 'eval_steps_per_second': 0.34, 'epoch': 22.0}
{'loss': 0.0581, 'learning_rate': 4.666666666666667e-06, 'epoch': 23.0}
{'eval_loss': 1.1517537832260132, 'eval_accuracy': 0.725, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.6818181818181818, 'eval_f1': 0.7317073170731707, 'eval_runtime': 8.3153, 'eval_samples_per_second': 4.81, 'eval_steps_per_second': 0.361, 'epoch': 23.0}
{'loss': 0.0457, 'learning_rate': 4.000000000000001e-06, 'epoch': 24.0}
{'eval_loss': 1.0863535404205322, 'eval_accuracy': 0.75, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.7142857142857143, 'eval_f1': 0.7500000000000001, 'eval_runtime': 8.3524, 'eval_samples_per_second': 4.789, 'eval_steps_per_second': 0.359, 'epoch': 24.0}
{'loss': 0.0485, 'learning_rate': 3.3333333333333333e-06, 'epoch': 25.0}
{'eval_loss': 1.0120292901992798, 'eval_accuracy': 0.775, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.75, 'eval_f1': 0.7692307692307692, 'eval_runtime': 9.0778, 'eval_samples_per_second': 4.406, 'eval_steps_per_second': 0.33, 'epoch': 25.0}
{'loss': 0.0476, 'learning_rate': 2.666666666666667e-06, 'epoch': 26.0}
{'eval_loss': 1.0610687732696533, 'eval_accuracy': 0.75, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.7142857142857143, 'eval_f1': 0.7500000000000001, 'eval_runtime': 8.2833, 'eval_samples_per_second': 4.829, 'eval_steps_per_second': 0.362, 'epoch': 26.0}
{'loss': 0.0309, 'learning_rate': 2.0000000000000003e-06, 'epoch': 27.0}
{'eval_loss': 1.062745451927185, 'eval_accuracy': 0.75, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.7142857142857143, 'eval_f1': 0.7500000000000001, 'eval_runtime': 8.2989, 'eval_samples_per_second': 4.82, 'eval_steps_per_second': 0.361, 'epoch': 27.0}
{'loss': 0.031, 'learning_rate': 1.3333333333333334e-06, 'epoch': 28.0}
{'eval_loss': 1.0370614528656006, 'eval_accuracy': 0.775, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.75, 'eval_f1': 0.7692307692307692, 'eval_runtime': 8.9584, 'eval_samples_per_second': 4.465, 'eval_steps_per_second': 0.335, 'epoch': 28.0}
{'loss': 0.0352, 'learning_rate': 6.666666666666667e-07, 'epoch': 29.0}
{'eval_loss': 1.0471603870391846, 'eval_accuracy': 0.775, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.75, 'eval_f1': 0.7692307692307692, 'eval_runtime': 8.2943, 'eval_samples_per_second': 4.823, 'eval_steps_per_second': 0.362, 'epoch': 29.0}
{'loss': 0.047, 'learning_rate': 0.0, 'epoch': 30.0}
{'eval_loss': 1.0430514812469482, 'eval_accuracy': 0.775, 'eval_recall': 0.7894736842105263, 'eval_precision': 0.75, 'eval_f1': 0.7692307692307692, 'eval_runtime': 8.3177, 'eval_samples_per_second': 4.809, 'eval_steps_per_second': 0.361, 'epoch': 30.0}
{'train_runtime': 3271.5738, 'train_samples_per_second': 6.602, 'train_steps_per_second': 0.413, 'train_loss': 0.21000376833809747, 'epoch': 30.0}
Accuracy: 70.00%
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
