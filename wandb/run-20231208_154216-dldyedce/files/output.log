
You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
bert.embeddings.word_embeddings.weight: Requires Grad - True
bert.embeddings.position_embeddings.weight: Requires Grad - True
bert.embeddings.token_type_embeddings.weight: Requires Grad - True
bert.embeddings.LayerNorm.weight: Requires Grad - True
bert.embeddings.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.0.attention.self.query.weight: Requires Grad - False
bert.encoder.layer.0.attention.self.query.bias: Requires Grad - False
bert.encoder.layer.0.attention.self.key.weight: Requires Grad - False
bert.encoder.layer.0.attention.self.key.bias: Requires Grad - False
bert.encoder.layer.0.attention.self.value.weight: Requires Grad - False
bert.encoder.layer.0.attention.self.value.bias: Requires Grad - False
bert.encoder.layer.0.attention.output.dense.weight: Requires Grad - False
bert.encoder.layer.0.attention.output.dense.bias: Requires Grad - False
bert.encoder.layer.0.attention.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.0.attention.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.0.intermediate.dense.weight: Requires Grad - False
bert.encoder.layer.0.intermediate.dense.bias: Requires Grad - False
bert.encoder.layer.0.output.dense.weight: Requires Grad - False
bert.encoder.layer.0.output.dense.bias: Requires Grad - False
bert.encoder.layer.0.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.0.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.1.attention.self.query.weight: Requires Grad - False
bert.encoder.layer.1.attention.self.query.bias: Requires Grad - False
bert.encoder.layer.1.attention.self.key.weight: Requires Grad - False
bert.encoder.layer.1.attention.self.key.bias: Requires Grad - False
bert.encoder.layer.1.attention.self.value.weight: Requires Grad - False
bert.encoder.layer.1.attention.self.value.bias: Requires Grad - False
bert.encoder.layer.1.attention.output.dense.weight: Requires Grad - False
bert.encoder.layer.1.attention.output.dense.bias: Requires Grad - False
bert.encoder.layer.1.attention.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.1.attention.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.1.intermediate.dense.weight: Requires Grad - False
bert.encoder.layer.1.intermediate.dense.bias: Requires Grad - False
bert.encoder.layer.1.output.dense.weight: Requires Grad - False
bert.encoder.layer.1.output.dense.bias: Requires Grad - False
bert.encoder.layer.1.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.1.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.2.attention.self.query.weight: Requires Grad - False
bert.encoder.layer.2.attention.self.query.bias: Requires Grad - False
bert.encoder.layer.2.attention.self.key.weight: Requires Grad - False
bert.encoder.layer.2.attention.self.key.bias: Requires Grad - False
bert.encoder.layer.2.attention.self.value.weight: Requires Grad - False
bert.encoder.layer.2.attention.self.value.bias: Requires Grad - False
bert.encoder.layer.2.attention.output.dense.weight: Requires Grad - False
bert.encoder.layer.2.attention.output.dense.bias: Requires Grad - False
bert.encoder.layer.2.attention.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.2.attention.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.2.intermediate.dense.weight: Requires Grad - False
bert.encoder.layer.2.intermediate.dense.bias: Requires Grad - False
bert.encoder.layer.2.output.dense.weight: Requires Grad - False
bert.encoder.layer.2.output.dense.bias: Requires Grad - False
bert.encoder.layer.2.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.2.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.3.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.3.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.3.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.3.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.3.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.3.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.3.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.3.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.3.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.3.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.3.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.3.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.3.output.dense.weight: Requires Grad - True
bert.encoder.layer.3.output.dense.bias: Requires Grad - True
bert.encoder.layer.3.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.3.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.4.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.4.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.4.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.4.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.4.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.4.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.4.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.4.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.4.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.4.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.4.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.4.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.4.output.dense.weight: Requires Grad - True
bert.encoder.layer.4.output.dense.bias: Requires Grad - True
bert.encoder.layer.4.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.4.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.5.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.5.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.5.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.5.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.5.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.5.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.5.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.5.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.5.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.5.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.5.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.5.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.5.output.dense.weight: Requires Grad - True
bert.encoder.layer.5.output.dense.bias: Requires Grad - True
bert.encoder.layer.5.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.5.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.6.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.6.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.6.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.6.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.6.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.6.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.6.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.6.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.6.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.6.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.6.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.6.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.6.output.dense.weight: Requires Grad - True
bert.encoder.layer.6.output.dense.bias: Requires Grad - True
bert.encoder.layer.6.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.6.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.7.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.7.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.7.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.7.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.7.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.7.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.7.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.7.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.7.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.7.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.7.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.7.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.7.output.dense.weight: Requires Grad - True
bert.encoder.layer.7.output.dense.bias: Requires Grad - True
bert.encoder.layer.7.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.7.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.8.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.8.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.8.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.8.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.8.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.8.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.8.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.8.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.8.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.8.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.8.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.8.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.8.output.dense.weight: Requires Grad - True
bert.encoder.layer.8.output.dense.bias: Requires Grad - True
bert.encoder.layer.8.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.8.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.9.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.9.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.9.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.9.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.9.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.9.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.9.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.9.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.9.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.9.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.9.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.9.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.9.output.dense.weight: Requires Grad - True
bert.encoder.layer.9.output.dense.bias: Requires Grad - True
bert.encoder.layer.9.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.9.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.10.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.10.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.10.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.10.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.10.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.10.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.10.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.10.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.10.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.10.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.10.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.10.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.10.output.dense.weight: Requires Grad - True
bert.encoder.layer.10.output.dense.bias: Requires Grad - True
bert.encoder.layer.10.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.10.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.11.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.11.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.11.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.11.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.11.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.11.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.11.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.11.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.11.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.11.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.11.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.11.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.11.output.dense.weight: Requires Grad - True
bert.encoder.layer.11.output.dense.bias: Requires Grad - True
bert.encoder.layer.11.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.11.output.LayerNorm.bias: Requires Grad - True
bert.pooler.dense.weight: Requires Grad - True
bert.pooler.dense.bias: Requires Grad - True
classifier.weight: Requires Grad - True
classifier.bias: Requires Grad - True
You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
bert.embeddings.word_embeddings.weight: Requires Grad - True
bert.embeddings.position_embeddings.weight: Requires Grad - True
bert.embeddings.token_type_embeddings.weight: Requires Grad - True
bert.embeddings.LayerNorm.weight: Requires Grad - True
bert.embeddings.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.0.attention.self.query.weight: Requires Grad - False
bert.encoder.layer.0.attention.self.query.bias: Requires Grad - False
bert.encoder.layer.0.attention.self.key.weight: Requires Grad - False
bert.encoder.layer.0.attention.self.key.bias: Requires Grad - False
bert.encoder.layer.0.attention.self.value.weight: Requires Grad - False
bert.encoder.layer.0.attention.self.value.bias: Requires Grad - False
bert.encoder.layer.0.attention.output.dense.weight: Requires Grad - False
bert.encoder.layer.0.attention.output.dense.bias: Requires Grad - False
bert.encoder.layer.0.attention.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.0.attention.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.0.intermediate.dense.weight: Requires Grad - False
bert.encoder.layer.0.intermediate.dense.bias: Requires Grad - False
bert.encoder.layer.0.output.dense.weight: Requires Grad - False
bert.encoder.layer.0.output.dense.bias: Requires Grad - False
bert.encoder.layer.0.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.0.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.1.attention.self.query.weight: Requires Grad - False
bert.encoder.layer.1.attention.self.query.bias: Requires Grad - False
bert.encoder.layer.1.attention.self.key.weight: Requires Grad - False
bert.encoder.layer.1.attention.self.key.bias: Requires Grad - False
bert.encoder.layer.1.attention.self.value.weight: Requires Grad - False
bert.encoder.layer.1.attention.self.value.bias: Requires Grad - False
bert.encoder.layer.1.attention.output.dense.weight: Requires Grad - False
bert.encoder.layer.1.attention.output.dense.bias: Requires Grad - False
bert.encoder.layer.1.attention.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.1.attention.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.1.intermediate.dense.weight: Requires Grad - False
bert.encoder.layer.1.intermediate.dense.bias: Requires Grad - False
bert.encoder.layer.1.output.dense.weight: Requires Grad - False
bert.encoder.layer.1.output.dense.bias: Requires Grad - False
bert.encoder.layer.1.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.1.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.2.attention.self.query.weight: Requires Grad - False
bert.encoder.layer.2.attention.self.query.bias: Requires Grad - False
bert.encoder.layer.2.attention.self.key.weight: Requires Grad - False
bert.encoder.layer.2.attention.self.key.bias: Requires Grad - False
bert.encoder.layer.2.attention.self.value.weight: Requires Grad - False
bert.encoder.layer.2.attention.self.value.bias: Requires Grad - False
bert.encoder.layer.2.attention.output.dense.weight: Requires Grad - False
bert.encoder.layer.2.attention.output.dense.bias: Requires Grad - False
bert.encoder.layer.2.attention.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.2.attention.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.2.intermediate.dense.weight: Requires Grad - False
bert.encoder.layer.2.intermediate.dense.bias: Requires Grad - False
bert.encoder.layer.2.output.dense.weight: Requires Grad - False
bert.encoder.layer.2.output.dense.bias: Requires Grad - False
bert.encoder.layer.2.output.LayerNorm.weight: Requires Grad - False
bert.encoder.layer.2.output.LayerNorm.bias: Requires Grad - False
bert.encoder.layer.3.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.3.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.3.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.3.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.3.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.3.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.3.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.3.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.3.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.3.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.3.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.3.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.3.output.dense.weight: Requires Grad - True
bert.encoder.layer.3.output.dense.bias: Requires Grad - True
bert.encoder.layer.3.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.3.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.4.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.4.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.4.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.4.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.4.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.4.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.4.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.4.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.4.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.4.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.4.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.4.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.4.output.dense.weight: Requires Grad - True
bert.encoder.layer.4.output.dense.bias: Requires Grad - True
bert.encoder.layer.4.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.4.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.5.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.5.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.5.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.5.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.5.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.5.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.5.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.5.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.5.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.5.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.5.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.5.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.5.output.dense.weight: Requires Grad - True
bert.encoder.layer.5.output.dense.bias: Requires Grad - True
bert.encoder.layer.5.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.5.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.6.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.6.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.6.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.6.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.6.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.6.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.6.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.6.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.6.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.6.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.6.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.6.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.6.output.dense.weight: Requires Grad - True
bert.encoder.layer.6.output.dense.bias: Requires Grad - True
bert.encoder.layer.6.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.6.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.7.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.7.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.7.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.7.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.7.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.7.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.7.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.7.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.7.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.7.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.7.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.7.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.7.output.dense.weight: Requires Grad - True
bert.encoder.layer.7.output.dense.bias: Requires Grad - True
bert.encoder.layer.7.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.7.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.8.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.8.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.8.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.8.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.8.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.8.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.8.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.8.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.8.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.8.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.8.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.8.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.8.output.dense.weight: Requires Grad - True
bert.encoder.layer.8.output.dense.bias: Requires Grad - True
bert.encoder.layer.8.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.8.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.9.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.9.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.9.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.9.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.9.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.9.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.9.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.9.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.9.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.9.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.9.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.9.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.9.output.dense.weight: Requires Grad - True
bert.encoder.layer.9.output.dense.bias: Requires Grad - True
bert.encoder.layer.9.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.9.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.10.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.10.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.10.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.10.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.10.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.10.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.10.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.10.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.10.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.10.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.10.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.10.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.10.output.dense.weight: Requires Grad - True
bert.encoder.layer.10.output.dense.bias: Requires Grad - True
bert.encoder.layer.10.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.10.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.11.attention.self.query.weight: Requires Grad - True
bert.encoder.layer.11.attention.self.query.bias: Requires Grad - True
bert.encoder.layer.11.attention.self.key.weight: Requires Grad - True
bert.encoder.layer.11.attention.self.key.bias: Requires Grad - True
bert.encoder.layer.11.attention.self.value.weight: Requires Grad - True
bert.encoder.layer.11.attention.self.value.bias: Requires Grad - True
bert.encoder.layer.11.attention.output.dense.weight: Requires Grad - True
bert.encoder.layer.11.attention.output.dense.bias: Requires Grad - True
bert.encoder.layer.11.attention.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.11.attention.output.LayerNorm.bias: Requires Grad - True
bert.encoder.layer.11.intermediate.dense.weight: Requires Grad - True
bert.encoder.layer.11.intermediate.dense.bias: Requires Grad - True
bert.encoder.layer.11.output.dense.weight: Requires Grad - True
bert.encoder.layer.11.output.dense.bias: Requires Grad - True
bert.encoder.layer.11.output.LayerNorm.weight: Requires Grad - True
bert.encoder.layer.11.output.LayerNorm.bias: Requires Grad - True
bert.pooler.dense.weight: Requires Grad - True
bert.pooler.dense.bias: Requires Grad - True
classifier.weight: Requires Grad - True
classifier.bias: Requires Grad - True
You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
0    My little brother is getting married next Satu...
1    I will be driving for 80-90 minutes across Lon...
2    We were planning to get dinner somewhere near ...
3    We are going to cannock chase with the mountai...
4    I will be getting up Tuesday morning to go int...
Name: q, dtype: object
{'train': (1476, 2), 'valid': (82, 2), 'test': (82, 2)}
                                                   sent label
0     My little brother is getting married next Satu...     1
1     I will be driving for 80-90 minutes across Lon...     1
2     We were planning to get dinner somewhere near ...     1
3     We are going to cannock chase with the mountai...     1
4     I will be getting up Tuesday morning to go int...     1
...                                                 ...   ...
1635  I'm going on a walk with a friend of mine and ...     0
1636  I am going to a museum in Manchester that focu...     0
1637  We used to be housemates and always watched th...     0
1638  The RAF 100 Bicycle Trail is a commemorative t...     0
1639  Going to meet up with friends. Have lunch and ...     0
[1640 rows x 2 columns]
                                                sent label
0  My little brother is getting married next Satu...     1
1  I will be driving for 80-90 minutes across Lon...     1
2  We were planning to get dinner somewhere near ...     1
3  We are going to cannock chase with the mountai...     1
4  I will be getting up Tuesday morning to go int...     1
(1640,)
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Maximum Token Limit: 1000000000000000019884624838656
