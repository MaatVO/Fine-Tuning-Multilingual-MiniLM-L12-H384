# Fine-Tuning Multilingual-MiniLM-L12-H384 for Lie Detection using Transfer Learning and LORA

![GitHub last commit](https://img.shields.io/github/last-commit/MaatVO/Fine-Tuning-Multilingual-MiniLM-L12-H384)
![GitHub issues](https://img.shields.io/github/issues/MaatVO/Fine-Tuning-Multilingual-MiniLM-L12-H384)

## Overview

This repository contains code for fine-tuning the Multilingual-MiniLM-L12-H384 language model from Microsoft, based on BART, for lie detection using transfer learning and LORA (Low-Rank Adaptation of Large Language Models). The pre-trained model can be found on the [Hugging Face Model Hub](https://huggingface.co/microsoft/Multilingual-MiniLM-L12-H384).

## Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
- [Dataset](#dataset)
- [Training](#training)
- [Evaluation](#evaluation)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)

## Installation

To set up the environment, you can use the following commands:

```bash
git clone https://github.com/MaatVO/Fine-Tuning-Multilingual-MiniLM-L12-H384.git
cd Fine-Tuning-Multilingual-MiniLM-L12-H384
pip install -r requirements.txt

#WIP
